* Ch 1
** Examples
- DrivenData: Datascience competitions to build a better world
** Volume, Velocity, Veraciy, Variety
** Privacy issues
- Require additional caution
** Categorize data by structure and volatility
- Structure or unstructured | data in motion or at rest
** small datasets handled by flat file databases
- For bigger ones it is crucial to model the relationships between data
  variables.
** Exponential increase in data required paradigm shift
- To meet up this increase we scale horizontally rather than vertically.
- Massive distributed storing and computation.
- i.e. NoSql and Hadoop
** Data volatility
- Data at rest: example facebook app, facebook cloud storing data generated
  by user first then analyzing it to act upon it later.
- Data in Motion: replace facebook app by an app interfacing with sensor.
** Evolution to Big data
- Data problem is not an electronics problem.
- Data scale is solved by scaling servers horizontally rather than
  vertically.
* Ch 2
** DIKW pyramid
*** Wiki
- It comes from Epistemology, theory of knowledge
- Data: Facts, Signals, ...
- Information: who, what, who, where, how many, when
*** Example
- Data - Collect temperature readings from multiple, geo-localized sensors.
- Information - Extract temporal and localization insights. It shows that temperatures are constantly rising globally.
- Knowledge - Compare multiple hypotheses and it becomes apparent that the rise appears to be caused by human activities, including greenhouse emissions.
- Wisdom - Work to reduce greenhouse gas emissions.
** Analytics
*** Example
- descriptive analytics - SF["Category"].value_counts()
- Predictive analytics - price going where in stock markets
- Prescriptive analytics - buy or sell in stock markets, even if price goes
  high volatility might not be suited if reward is not enough for the risk.
** Role of time in Data
- Which stocks will most likely have the highest daily gain based on trading in the last hour?
- What is the best way to route delivery trucks this afternoon, based on morning sales, existing inventory, and current traffic reports?
- What maintenance is required for this airplane, based on performance data generated during the last flight?
- i.e. Stock markets, time is very vital especially doing forex scalping.
** ETL 
- Extract, Transform and Load
* Ch 3
** Exploratory Data Analaysis
- Supervised Descriptive statistics
- Takes place after Data Gathering
- Takes place before Prepare Data step
- Look for interesting patterns in Data
** What is Data
- Data is normally a representation of characeteristics or features of a
  phenomenon.
- This features are modeled as variables.
** Statistics
- Descriptive Statistics: tells about sample.
- Inferential Statistics: tries to generalize from sample to population.
- Describe data using statistical centrality properties like mean, median,
  mode. 
  - Describe data using dispersion properties
  - Describe data using correlation between different variables.
* Ch 4 
** learning process
- Movie Recommendation Example
 - Throw all data you know about movie
 - It is not our role to know if this info data is relevant or not
 - Learning process decides the impact of this piece of info we throw
** Supervised vs Unsupervised
** Regression vs Classification
- They are both Supervised. 
- Go in details on Regression.
- Regression gives a mathematical relation between variables.
** SVM
- Example for support vector machine
 - A circle border that seperates two classes is solved using SVM
** Validation
** Reliability
- Ensuring that inference model is repeatable
** Error Analysis
- Classify error
- From where does it orginate
- How to mitigate
* Ch 5
** Know the audience
** Different types of realizations to convey the message
** RULE: Do not add to the chart components you do not need in presentation
* Ch 6
** Scale up
- Hardware architectures & Software tools to scale up big data and analytic
  solutions.
** Architecture
- Storage Layer
- Data Processing Paradigm
- One node fails every 3 years -> one node fails every day in cluster of 1000
  nodes.
- GFS key components
 - Failures are a norm not exception.
 - write once read many
- HDFS is an open source implementation of GFS
- HDFS server roles
 - Namenode
 - Datanode
** Hadoop
** Spark
*** MapReduce shortcomings
- Underutilization of cluster memory
- Redundant disk I/O
*** Spark Response
- RDD abstraction with rich API
- In-memory distributed computation platform
- Spark abstracts away the execution detail hence improve developer
  productivity and code readability.
- Caching is supported, needed to be applied by developer.
 - example of two RDDs executed from one common parent RDD.
 - This common parent needs to be cached otherwise the source file will be
   loaded from HDFS twice.
*** Need for RDD
- computations (K-means, PageRank, ...) known to user code but unknown to the
  framework hence the framework does not optimize it that is in case of
  mapreduce.
- MapReduce is inefficient for iterative computations because it guarantees
  durability of the result by reading and writing dataset on every iteration.
  As persisting datasets over and over again requires a lot of I/O time.
- Transformation : rdd.filter(lambda x: x%2 == 0) => rdd2
 - Hence rdd2 dependant on rdd
- Transformations are written by Driver and action is taken on them by
  executors.
*** Packages over Spark Core
- Spark SQL
- Spark Streaming
- Spark GraphX
- Spark MLlib
** Data Engineering
- This is an independent field in Big data.
** Big data Pipeline
- Example: Count population in Egypt above 21 including foreigners.
 - All ministries possess databases
  - i.e. Ministry of Higher Education includes foreigner students.
  - i.e. Ministry of Health and Population includes born and deceased in
    Egypt.
  - i.e. Ministry of Interior includes data about foreign workers.
  - etc...
 - Kafka subscribes to data published by all ministries.
  - Data is stored via casandra
  - Spark computes the count considering redunduncies.
